{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AMATH 515 Homework 2\n",
    "\n",
    "*Homework Instruction*: Please fill in the gaps in this template where commented as `TODO`. When submitting, make sure your file is still named `515Hw2_Coding.ipynb` -- that's what Gradescope will be looking for. There is no need to submit `hw2_supp.py`, it will already be on the server when you submit your work. You'll have **10 attempts** to pass the tests. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# require numpy module\n",
    "import numpy as np\n",
    "from numpy.linalg import norm\n",
    "from numpy.linalg import solve\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import supplementary functions for Homework 1\n",
    "import sys\n",
    "sys.path.insert(0, './')\n",
    "from hw2_supp import *\n",
    "\n",
    "import matplotlib\n",
    "matplotlib.use('AGG')\n",
    "# Have to manually set barebones backend or else autograder is mad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can change this seed if you want to test that your program works with different random data,\n",
    "# but please set it back to 123 when you submit your work.\n",
    "seed = 123"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent Solver\n",
    "\n",
    "Recall the gradient descent algorithm we learned in class and complete the gradient descent solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeWithGD(x0, func, grad, step_size, tol=1e-6, max_iter=1000, use_line_search=False):\n",
    "    \"\"\"\n",
    "    Optimize with Gradient Descent\n",
    "    \n",
    "    input\n",
    "    -----\n",
    "    x0 : array_like\n",
    "        Starting point for the solver.\n",
    "    func : function\n",
    "        Takes x and return the function value.\n",
    "    grad : function\n",
    "        Takes x and returns the gradient of \"func\".\n",
    "    step_size : float or None\n",
    "        If it is a float number and `use_line_search=False`, it will be used as the step size.\n",
    "        Otherwise, line search will be used\n",
    "    tol : float, optional\n",
    "        Gradient tolerance for terminating the solver.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iterations for terminating the solver.\n",
    "    use_line_search : bool, optional\n",
    "        When it is true a line search will be used, other wise `step_size` has to be provided.\n",
    "        \n",
    "    output\n",
    "    ------\n",
    "    x : array_like\n",
    "        Final solution\n",
    "    obj_his : array_like\n",
    "        Objective function's values convergence history\n",
    "    err_his : array_like\n",
    "        Norm of gradient convergence history\n",
    "    exit_flag : int\n",
    "        0, norm of gradient below `tol`\n",
    "        1, exceed maximum number of iteration\n",
    "        2, line search fail\n",
    "        3, other\n",
    "    \"\"\"\n",
    "    # safeguard\n",
    "    if not use_line_search and step_size is None:\n",
    "        print('Please specify the step_size or use the line search.')\n",
    "        return x0, np.array([]), np.array([]), 3\n",
    "    \n",
    "    # initial step\n",
    "    x = np.copy(x0)\n",
    "    g = grad(x)\n",
    "    #\n",
    "    obj = func(x)\n",
    "    err = norm(g)\n",
    "    #\n",
    "    obj_his = np.zeros(max_iter + 1)\n",
    "    err_his = np.zeros(max_iter + 1)\n",
    "    #\n",
    "    obj_his[0] = obj\n",
    "    err_his[0] = err\n",
    "    \n",
    "    # start iterations\n",
    "    iter_count = 0\n",
    "    while err >= tol:\n",
    "        if use_line_search:\n",
    "            step_size = lineSearch(x, g, g, func)\n",
    "        #\n",
    "        # if line search fail step_size will be None\n",
    "        if step_size is None:\n",
    "            print('Gradient descent line search fail.')\n",
    "            return x, obj_his[:iter_count+1], err_his[:iter_count+1], 2\n",
    "        #\n",
    "        # gradient descent step\n",
    "        #####\n",
    "        # TODO: with given step_size, complete gradient descent step\n",
    "        x = x - step_size*g\n",
    "        #####\n",
    "        #\n",
    "        # update function and gradient\n",
    "        g = grad(x)\n",
    "        #\n",
    "        obj = func(x)\n",
    "        err = norm(g)\n",
    "        #\n",
    "        iter_count += 1\n",
    "        obj_his[iter_count] = obj\n",
    "        err_his[iter_count] = err\n",
    "        #\n",
    "        # check if exceed maximum number of iteration\n",
    "        if iter_count >= max_iter:\n",
    "            print('Gradient descent reach maximum number of iteration.')\n",
    "            return x, obj_his[:iter_count+1], err_his[:iter_count+1], 1\n",
    "    #\n",
    "    return x, obj_his[:iter_count+1], err_his[:iter_count+1], 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Newton's Solver\n",
    "\n",
    "Recall Newton method we learned from in class and complete Newton solver."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimizeWithNT(x0, func, grad, hess, tol=1e-6, max_iter=100):\n",
    "    \"\"\"\n",
    "    Optimize with Newton's Method\n",
    "    \n",
    "    input\n",
    "    -----\n",
    "    x0 : array_like\n",
    "        Starting point for the solver.\n",
    "    func : function\n",
    "        Takes x and return the function value.\n",
    "    grad : function\n",
    "        Takes x and return the gradient.\n",
    "    hess : function\n",
    "        Input x and return the Hessian matrix.\n",
    "    tol : float, optional\n",
    "        Gradient tolerance for terminating the solver.\n",
    "    max_iter : int, optional\n",
    "        Maximum number of iteration for terminating the solver.\n",
    "        \n",
    "    output\n",
    "    ------\n",
    "    x : array_like\n",
    "        Final solution\n",
    "    obj_his : array_like\n",
    "        Objective function value convergence history\n",
    "    err_his : array_like\n",
    "        Norm of gradient convergence history\n",
    "    exit_flag : int\n",
    "        0, norm of gradient below `tol`\n",
    "        1, exceed maximum number of iteration\n",
    "        2, others\n",
    "    \"\"\"\n",
    "    # initial step\n",
    "    x = np.copy(x0)\n",
    "    g = grad(x)\n",
    "    H = hess(x)\n",
    "    #\n",
    "    obj = func(x)\n",
    "    err = norm(g)\n",
    "    #\n",
    "    obj_his = np.zeros(max_iter + 1)\n",
    "    err_his = np.zeros(max_iter + 1)\n",
    "    #\n",
    "    obj_his[0] = obj\n",
    "    err_his[0] = err\n",
    "    \n",
    "    # start iterations\n",
    "    iter_count = 0\n",
    "    while err >= tol:\n",
    "        # Newton step\n",
    "        #####\n",
    "        # TODO: complete Newton step\n",
    "        d = solve(H, g)\n",
    "        step_size = lineSearch(x, g, d, func)\n",
    "        x = x -  step_size * d\n",
    "        #####\n",
    "        #\n",
    "        # update function, gradient and Hessian\n",
    "        g = grad(x)\n",
    "        H = hess(x)\n",
    "        #\n",
    "        obj = func(x)\n",
    "        err = norm(g)\n",
    "        #\n",
    "        iter_count += 1\n",
    "        obj_his[iter_count] = obj\n",
    "        err_his[iter_count] = err\n",
    "        #\n",
    "        # check if we exceeded maximum number of iterations\n",
    "        if iter_count >= max_iter:\n",
    "            print('Gradient descent reach maximum number of iteration.')\n",
    "            return x, obj_his[:iter_count+1], err_his[:iter_count+1], 1\n",
    "    #\n",
    "    return x, obj_his[:iter_count+1], err_his[:iter_count+1], 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Test\n",
    "\n",
    "Consider a simple test problem,\n",
    "\n",
    "$$\n",
    "\\min_x~~f(x) \\triangleq \\frac{1}{2} \\|x - b\\|^2\n",
    "$$\n",
    "\n",
    "We can easily calculate the gradient and Hessian of $f$,\n",
    "\n",
    "$$\n",
    "\\nabla f(x) = x - b, \\quad \\nabla^2 f(x) = I\n",
    "$$\n",
    "\n",
    "where $I$ is an identity matrix, and we know that $x^* = b$ is the solution.\n",
    "\n",
    "We will use this function as a simple test to check whether gradient descent and Newton method are implemented right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create b\n",
    "b = np.array([1.0, 2.0, 3.0])\n",
    "# define test function\n",
    "def test_func(x):\n",
    "    return 0.5*sum((x - b)**2)\n",
    "# define test gradient\n",
    "def test_grad(x):\n",
    "    return x - b\n",
    "# define test Hessian\n",
    "def test_hess(x):\n",
    "    return np.eye(b.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradient Descent: OK\n"
     ]
    }
   ],
   "source": [
    "# test gradient descent\n",
    "x0_gd = np.zeros(b.size)\n",
    "#\n",
    "x_gd, obj_his_gd, err_his_gd, exit_flag_gd = optimizeWithGD(x0_gd, test_func, test_grad, 1.0)\n",
    "# check if the solution is correct\n",
    "err_gd = norm(x_gd - b)\n",
    "#\n",
    "if err_gd < 1e-6:\n",
    "    print('Gradient Descent: OK')\n",
    "else:\n",
    "    print('Gradient Descent: Err')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Newton: OK\n"
     ]
    }
   ],
   "source": [
    "# test Newton's method\n",
    "x0_nt = np.zeros(b.size)\n",
    "#\n",
    "x_nt, obj_his_nt, err_his_nt, exit_flag_nt = optimizeWithNT(x0_nt, test_func, test_grad, test_hess)\n",
    "# check if the solution is correct\n",
    "err_nt = norm(x_nt - b)\n",
    "#\n",
    "if err_nt < 1e-6:\n",
    "    print('Newton: OK')\n",
    "else:\n",
    "    print('Newton: Err')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Recall the logistic regression model\n",
    "$$\n",
    "f(x) = \\sum_{i=1}^m\\left\\{\\log\\left[1+\\exp(\\langle a_i,x\\rangle)\\right]-b_i\\langle a_i,x\\rangle\\right\\} + \\frac{\\lambda}{2}\\|x\\|^2.\n",
    "$$\n",
    "\n",
    "Calculate the gradient and Hessian of this function and define them below using the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix a random seed\n",
    "np.random.seed(seed)\n",
    "# set dimensions and create some random data\n",
    "m_lgt = 500\n",
    "n_lgt = 50\n",
    "A_lgt = 0.3*np.random.randn(m_lgt, n_lgt)\n",
    "x_lgt = np.random.randn(n_lgt)\n",
    "b_lgt = sampleLGT(x_lgt, A_lgt)\n",
    "lam_lgt = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implement logistic function, gradient and Hessian\n",
    "def lgt_func(x):\n",
    "    sum = 0\n",
    "    for i in range(0, m_lgt):\n",
    "        exp = np.exp(np.dot(A_lgt[i,:], x))\n",
    "        ai = A_lgt[i,:]\n",
    "        sum += np.log(1 + exp) - b_lgt[i]*np.dot(ai,x)\n",
    "\n",
    "    sum += lam_lgt/2 * np.linalg.norm(x)**2\n",
    "    return sum\n",
    "#\n",
    "def lgt_grad(x):\n",
    "    sum = np.zeros((n_lgt,))\n",
    "    for i in range(0, m_lgt):\n",
    "        exp = np.exp(np.dot(A_lgt[i,:], x))\n",
    "        ai = A_lgt[i,:]\n",
    "        sum += exp*ai/(1+exp) - ai*b_lgt[i]\n",
    "\n",
    "    sum += lam_lgt * x\n",
    "    return sum\n",
    "#\n",
    "def lgt_hess(x):\n",
    "    sum = np.zeros((n_lgt, n_lgt))\n",
    "    for i in range(0, m_lgt):\n",
    "        exp = np.exp(np.dot(A_lgt[i,:], x))\n",
    "        ai = A_lgt[i,:]\n",
    "        sum += (exp)/(1 + exp)**2  * np.linalg.outer(ai,ai)\n",
    "\n",
    "    sum += lam_lgt * np.eye(n_lgt)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==GRADED==#\n",
    "# No need to change anything in this cell.\n",
    "x_test = 3*np.ones(n_lgt)\n",
    "# We test the values of these functions at a sample point (3, 3, ..., 3)\n",
    "lgt_func_test = lgt_func(x_test)\n",
    "lgt_grad_test = lgt_grad(x_test)\n",
    "lgt_hess_test = lgt_hess(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform step size for gradient descent or should we use line search?\n",
    "#####\n",
    "# TODO: if there is a uniform stepsize, set up step_size_lgt =\n",
    "# otherwise set use_line_search=True\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply gradient descent solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_lgt_gd = np.zeros(n_lgt)\n",
    "#==GRADED==#\n",
    "# No need to change anything in this cell.\n",
    "x_lgt_gd, obj_his_lgt_gd, err_his_lgt_gd, exit_flag_lgt_gd = \\\n",
    "    optimizeWithGD(x0_lgt_gd, lgt_func, lgt_grad, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Gradient Descent on Logistic Regression')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(obj_his_lgt_gd)\n",
    "ax[0].set_title('function value')\n",
    "ax[1].semilogy(err_his_lgt_gd)\n",
    "ax[1].set_title('norm of the gradient')\n",
    "fig.suptitle('Gradient Descent on Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Newton's solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_lgt_nt = np.zeros(n_lgt)\n",
    "#==GRADED==#\n",
    "# No need to change anything in this cell.\n",
    "x_lgt_nt, obj_his_lgt_nt, err_his_lgt_nt, exit_flag_lgt_nt = \\\n",
    "    optimizeWithNT(x0_lgt_nt, lgt_func, lgt_grad, lgt_hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, \"Newton's Method on Logistic Regression\")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(obj_his_lgt_nt)\n",
    "ax[0].set_title('function value')\n",
    "ax[1].semilogy(err_his_lgt_nt)\n",
    "ax[1].set_title('norm of the gradient')\n",
    "fig.suptitle('Newton\\'s Method on Logistic Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poisson Regression\n",
    "\n",
    "Recall the Poisson regression model\n",
    "$$\n",
    "f(x)=\\sum_{i=1}^m\\left[\\exp(\\langle a_i,x\\rangle)-b_i\\langle a_i,x\\rangle\\right]+\\frac{\\lambda}{2}\\|x\\|^2\n",
    "$$\n",
    "\n",
    "Calculate the gradient and Hessian of this function and define them below using the given data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix a random seed\n",
    "np.random.seed(seed)\n",
    "# set the dimensions and create some data\n",
    "m_psn = 500\n",
    "n_psn = 50\n",
    "A_psn = 0.3*np.random.randn(m_psn, n_psn)\n",
    "x_psn = np.random.randn(n_psn)\n",
    "b_psn = samplePSN(x_psn, A_psn)\n",
    "lam_psn = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define function, gradient and Hessian\n",
    "def psn_func(x):\n",
    "    sum = 0\n",
    "    for i in range(0, m_psn):\n",
    "        ai = A_psn[i,:]\n",
    "        exp = np.exp(np.dot(ai, x))\n",
    "        \n",
    "        sum += exp - b_psn[i]*np.dot(ai,x)\n",
    "\n",
    "    sum += lam_psn/2 * np.linalg.norm(x)**2\n",
    "    return sum\n",
    "#\n",
    "def psn_grad(x):\n",
    "    sum = np.zeros((n_psn))\n",
    "    for i in range(0, m_psn):\n",
    "        ai = A_psn[i,:]\n",
    "        exp = np.exp(np.dot(ai, x))\n",
    "        \n",
    "        sum += exp*ai - b_psn[i]*ai\n",
    "\n",
    "    sum += lam_psn * x\n",
    "    return sum\n",
    "#\n",
    "def psn_hess(x):\n",
    "    sum = np.zeros((n_psn,n_psn))\n",
    "    for i in range(0, m_psn):\n",
    "        ai = A_psn[i,:]\n",
    "        aiT = ai.T\n",
    "        exp = np.exp(np.dot(ai, x))\n",
    "        \n",
    "        sum += exp * np.linalg.outer(ai,ai)\n",
    "\n",
    "    sum += lam_psn*np.eye(n_psn)\n",
    "    return sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==GRADED==#\n",
    "# No need to change anything in this cell.\n",
    "x_test = 2*np.ones(n_psn)\n",
    "psn_func_test = psn_func(x_test)\n",
    "psn_grad_test = psn_grad(x_test)\n",
    "psn_hess_test = psn_hess(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uniform step size for gradient descent or should we use line search?\n",
    "#####\n",
    "# TODO: if there is a uniform stepsize, set up step_size_lgt = \n",
    "# otherwise set use_line_search=True\n",
    "#####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply gradient descent solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_psn_gd = np.zeros(n_lgt)\n",
    "#==GRADED==#\n",
    "# No need to change anything in this cell.\n",
    "x_psn_gd, obj_his_psn_gd, err_his_psn_gd, exit_flag_psn_gd = \\\n",
    "    optimizeWithGD(x0_psn_gd, psn_func, psn_grad, None, use_line_search=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, 'Gradient Descent on Poisson Regression')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(obj_his_psn_gd)\n",
    "ax[0].set_title('function value')\n",
    "ax[1].semilogy(err_his_psn_gd)\n",
    "ax[1].set_title('norm of the gradient')\n",
    "fig.suptitle('Gradient Descent on Poisson Regression')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply Newton's solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0_psn_nt = np.zeros(n_lgt)\n",
    "#==GRADED==#\n",
    "# No need to change anything in this cell.\n",
    "x_psn_nt, obj_his_psn_nt, err_his_psn_nt, exit_flag_psn_nt = \\\n",
    "    optimizeWithNT(x0_psn_nt, psn_func, psn_grad, psn_hess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0.98, \"Newton's Method on Poisson Regression\")"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# plot result\n",
    "fig, ax = plt.subplots(1, 2, figsize=(12,5))\n",
    "ax[0].plot(obj_his_psn_nt)\n",
    "ax[0].set_title('function value')\n",
    "ax[1].semilogy(err_his_psn_nt)\n",
    "ax[1].set_title('norm of the gradient')\n",
    "fig.suptitle('Newton\\'s Method on Poisson Regression')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
